{
  "0": {
    "id": "0",
    "title": "About Us",
    "content": "About Add info about the product. Add info about the team. Add info about the contact.",
    "url": "/pages/about.html",
    "relUrl": "/pages/about.html"
  },
  "1": {
    "id": "1",
    "title": "SQLFlow: Bridging Data and AI",
    "content": "",
    "url": "/",
    "relUrl": "/"
  },
  "2": {
    "id": "2",
    "title": "Build TensorFlow from Source Code using Docker",
    "content": "Build TensorFlow from Source Code using Docker To contribute to TensorFlow, we need to build TensorFlow from source code. The official guide is great. However, it interleaves the native building process and that using Docker and makes it confusing because packages needed by the former are not by the latter. Also, we found some useful tricks to start Docker containers in practices that are not in the official guide. Hence this document. Build the Pip Package in TensorFlow Development Container On either Mac or Linux, or any other OS, we don’t have to install and configure the building tools; instead, we can use a Docker image where all tools have been installed and properly configured. Get the Docker image containing all the building tools: docker pull tensorflow/tensorflow:latest-devel Then, let’s get the source code. On any OS, please install git using the native package manager. For example, on Ubuntu, please sudo apt-get install git or, on Mac, brew install git Then, use the git just installed, let’s clone tensorflow source code: git clone --recursive https://github.com/tensorflow/tensorflow cd tensorflow By default, we will be on the master branch. Feel free to do you change in your feature branches, or switch to a release branch, for example: git checkout v1.11.0 git checkout -b v1.11.0 Then, let us start a Docker container running the tensorflow/tensorflow:latest-devel image: docker run --rm -it -w /tensorflow -v $PWD:/tensorflow -v $HOME/.cache:/root/.cache -e &quot;HOST_PERMS=$(id -u):$(id -g)&quot; tensorflow/tensorflow:latest-devel /bin/bash -w /tensorflow brings us to the /tensorflow directory in the container once after we start it. -v $PWD:/tensorflow maps the current directory, which is the just cloned TensorFlow source directory on the host, to /tensorflow in the container. -v $HOME/.cache:/root/.cache maps the Bazel temporary directory on the host into the container, so the intermediate files generated by Bazel running in the container are actually saved on the host. This allows us to interrupt the container during the build and restart it later to resume the building. e &quot;HOST_PERMS=$(id -u):$(id -g)&quot; passes the user identification on the host into the container as an environment variable. We can reset the mode of files generated in the container to this user identity. From now on, we will be working in the container. Let us first configure the building: ./configure Usually, I would simply choose all the default options by hitting enter all the way down. Build the pip package builder: bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package Build the pip package and save it into /tensorflow: ./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tensorflow Change the generated wheel file into the usual mode as on the host: chown $HOST_PERMS /mnt/tensorflow-*.whl Install the Pip Pacakge in TensorFlow Development Container Let’s now try the new pip package. First, we need to uninstall the current tensorflow pip package, then we install the newly built one: pip uninstall tensorflow pip install /tensorflow/tensorflow-*.whl Now, we can verify if the new package works. First, we need to switch to a directory out from /tensorflow, so we don’t import from the source directory: cd /tmp # other directories also work then, we can try import the newly installed tensorflow package and verify it: python &gt;&gt;&gt; import tensorflow &gt;&gt;&gt; print(tensorflow.__version__) Now, let us quit from Python and from the Docker container. We should see the tensorflow-*.whl file on the host in the current directory. Install the Pip Package in a Clean Python Package After we quit from the development container, we should see the wheel file in the TensorFlow source directory on the host. Now, we can start a Python container and install the Pip package in it. Start the Python container docker run --rm -it -v $PWD:/tensorflow python:2.7 bash Install the pip pacakge in the container pip install /tensorflow/tensorflow*.whl Try TensorFlow by starting Python in the container python &gt;&gt;&gt; import tensorflow as tf &gt;&gt;&gt; print(tf.__version__)",
    "url": "/doc/build-tensorflow.html",
    "relUrl": "/doc/build-tensorflow.html"
  },
  "3": {
    "id": "3",
    "title": "Canonical Development Environment",
    "content": "Canonical Development Environment Referring to this example, we create a canonical development environment for Go and Python programmers using Docker. Editing on Host When we use this Docker image for daily development work, the source code relies on the host computer instead of the container. The source code includes this repo and all its dependencies, for example, the Go package google.golang.org/grpc. Code-on-the-host allows us to run our favorite editors (Emacs, VIM, Eclipse, and more) on the host. Please free to rely on editors add-ons to analyze the source code for auto-completion. Building in Container We build a Docker image that contains development tools: The Python interpreter The Go compiler The protobuf compiler The protobuf to Go compiler extension The protobuf to Python compiler extension Because this repo contains Go code, please make sure that you have the directory structure required by Go. On my laptop computer, I have export GOPATH=$HOME/go You could have your $GOPATH pointing to any directory you like. Given $GOPATH$ set, we could git clone the source code of our project by running: go get github.com/sql-machine-learning/sqlflow Change the directory to our project root, and we can use go get to retrieve and update Go dependencies. cd $GOPATH/src/github.com/sql-machine-learning/sqlflow go get -u -t ./... Note -t instructs get to also download the packages required to build the tests for the specified packages. As all Git users would do, we run git pull from time to time to sync up with others’ work. If somebody added new dependencies, we might need to run go -u ./... after git pull to update dependencies. To build this project, we need the protobuf compiler, Go compiler, Python interpreter, gRPC extension to the protobuf compiler. To ease the installation and configuration of these tools, we provided a Dockerfile to install them into a Docker image. To build the Docker image: docker build -t sqlflow:dev -f Dockerfile.dev . Development Build and Test We build and test the project inside the docker container. To run the container, we need to map the $GOPATH directory on the host into the /go directory in the container, because the Dockerfile configures /go as the $GOPATH in the container: docker run --rm -it -v $GOPATH:/go -w /go/src/github.com/sql-machine-learning/sqlflow sqlflow:dev bash Inside the Docker container, start a MySQL server in the background service mysql start&amp; run all the tests as go generate ./... go install ./... go test -v ./... where go generate invokes the protoc command to translate server/sqlflow.proto into server/sqlflow.pb.go and go test -v builds and run unit tests. Release The above build process currently generates two binary files in $GOPATH/bin on the host. To package them into a Docker image, please run docker build -t sqlflow -f ./Dockerfile $GOPATH/bin To publish the released Docker image to our official DockerHub docker tag sqlflow sqlflow/sqlflow:latest docker push sqlflow/sqlflow:latest Demo: Command line Prompt The demo requires a MySQL server instance with populated data. If we don’t, we could follow example/datasets/README.md to start one on the host. After setting up MySQL, run the following inside the Docker container go run cmd/demo/demo.go --db_user root --db_password root --db_address host.docker.internal:3306 You should be able to see the following prompt sqlflow&gt;",
    "url": "/doc/build.html",
    "relUrl": "/doc/build.html"
  },
  "4": {
    "id": "4",
    "title": "Closing the producer goroutine from the consumer",
    "content": "Closing the producer goroutine from the consumer The producer-and-consumer pattern is well used in Go concurrent programming. When the consumer stops, we want to gracefully stop the producer as well. Problem When a gRPC server receives a streaming request, it usually calls a function that returns a channel, reads the result from that channel and send the result to the client one by one. Take the following code for instance: upon receiving a request, the main goroutine Service calls launchJob. launchJob starts a separate goroutine as an anonymous function call and returns a channel. In the anonymous function, items will be sent to channel. And Service on the otherside of the channel will reads from it. func Service(req *Request, stream *StreamResponse) error { result := launchJob(req.Content) for r := range result { if e := stream.Send(result); e != nil { // should we signal the running goroutine so it will stop sending? return e } } } func launchJob(content string) chan Item { c := make(chan Item) go func() { defer close(c) acquireScarceResources() defer releaseScarceResources() ... // if stream.Send(result) returns an error and the Service returns, this will be blocked c &lt;- Item{} ... }() return c } There is a major problem in this implementation. As pointed out by the comment, if the Send in Service returns an error, the Service function will return, leaving the anonymous function being blocked on c &lt;- Item{} forever. This problem is important because the leaking goroutine usually owns scarce system resources such as network connection and memory. Solution: pipeline explicit cancellation Inspired by this blog post section Explicit cancellation, we can signal the cancellation via closing on a separate channel. And we can follow the terminology as io.Pipe. package sql import ( &quot;errors&quot; ) var ErrClosedPipe = errors.New(&quot;pipe: write on closed pipe&quot;) // pipe follows the design at https://blog.golang.org/pipelines // - wrCh: chan for piping data // - done: chan for signaling Close from Reader to Writer type pipe struct { wrCh chan interface{} done chan struct{} } // PipeReader reads real data type PipeReader struct { p *pipe } // PipeWriter writes real data type PipeWriter struct { p *pipe } // Pipe creates a synchronous in-memory pipe. // // It is safe to call Read and Write in parallel with each other or with Close. // Parallel calls to Read and parallel calls to Write are also safe: // the individual calls will be gated sequentially. func Pipe() (*PipeReader, *PipeWriter) { p := &amp;pipe{ wrCh: make(chan interface{}), done: make(chan struct{})} return &amp;PipeReader{p}, &amp;PipeWriter{p} } // Close closes the reader; subsequent writes to the func (r *PipeReader) Close() { close(r.p.done) } // ReadAll returns the data chan. The caller should // use it as `for r := range pr.ReadAll()` func (r *PipeReader) ReadAll() chan interface{} { return r.p.wrCh } // Close closes the writer; subsequent ReadAll from the // read half of the pipe will return a closed channel. func (w *PipeWriter) Close() { close(w.p.wrCh) } // Write writes the item to the underlying data stream. // It returns ErrClosedPipe when the data stream is closed. func (w *PipeWriter) Write(item interface{}) error { select { case w.p.wrCh &lt;- item: return nil case &lt;-w.p.done: return ErrClosedPipe } } And the consumer and producer be can implemented as func Service(req *Request, stream *StreamResponse) error { pr := launchJob(req.Content) defer pr.Close() for r := range pr.ReadAll() { if e := stream.Send(r); e != nil { return e } } } func launchJob(content string) PipeReader { pr, pw := Pipe() go func() { defer pw.Close() if err := pw.Write(Item{}); err != nil { return } } return pr } Further Reading Google Form: Channel send timeout Go by Example: Timeouts Google I/O 2013 - Advanced Go Concurrency Patterns Go Concurrency Patterns Talk Go Concurrency Patterns: Pipelines and cancellation",
    "url": "/doc/close_producer_from_consumer.html",
    "relUrl": "/doc/close_producer_from_consumer.html"
  },
  "5": {
    "id": "5",
    "title": "Compatibility with Various SQL Engines",
    "content": "Compatibility with Various SQL Engines SQLFlow interacts with SQL engines like MySQL and Hive, while different SQL engines use variants of SQL syntax, it is important for SQLFlow to have an abstraction layer that hides such differences. SQLFlow calls Go’s standard database API. The submitter programs generated by SQLFlow call Python’s database API. Both APIs abstract the interface to various SQL engines; however, they are insufficient for SQLFlow to work. In this document, we examine all interactions between SQLFlow and the SQL engine so to identify what SQLFlow authors have to abstract in addition to calling Go’s and Python’s database APIs. Data Operations in Go Data Retrieval The basic idea of SQLFlow is to extend the SELECT statement of SQL to have the TRAIN and PREDICT clauses. For more discussion, please refer to the syntax design. SQLFlow translates such “extended SQL statements” into submitter programs, which forward the part from SELECT to TRAIN or PREDICT, which we call the “standard part”, to the SQL engine. SQLFlow also accepts the SELECT statement without TRAIN or PREDICt clauses and would forward such “standard statements” to the engine. It is noticeable that the “standard part” or “standard statements” are not standardized. For example, various engines use different syntax for joining. MySQL: SELECT pet.name, comment FROM pet, event WHERE pet.name =event.name; with keyword WHERE . Hive: SELECT pet.name, comment FROM pet JOIN event ON (pet.name =event.name) with keyword JOIN and ON. ODPS and SQLite use either INNER JOIN or OUTER JOIN. Fortunately, as SQLFlow forwards the above parts to the engine, it doesn’t have to care much about the differences above. Metadata Retrieval To verify the semantics of users’ inputs, SQLFlow needs to retrieve the schema of tables. For example, the input might be SELECT name, age, income FROM employee TRAIN DNNRegressor WITH hidden_layers=[10,50,10] COLUMN name, agee LABEL income; In the above example, the user misspelled the field name age in the COLUMN clause as “agee”. SQLFlow must be able to find that out. To do that, SQLFlow needs to query the field names from the SQL engine. However, different engines use various syntax. For example: MySQL: DESCRIBE/DESC employee; Hive: DESCRIBE FORMATTED employee; ODPS: DESC employee; SQLite: PRAGMA table_info([employee]); The returned data format varies too. Our solution to avoid such differences is not-to-use-them; instead, SQLFlow retrieves the table schema by running a query like SELECT * FROM employee LIMIT 1; and inferring field types using the mechanism called DatabaseTypeName provided by SQL engines drivers beneath the Go’s standard database API. Prepare Prediction Table A SQLFlow prediction job writes its prediction results into a table. It prepares the prediction table by Dropping previous prediction table DROP TABLE IF EXISTS my_table; Creating table with schema CREATE TABLE my_table (name1, type1, name2 type2); Most SQL engines, including MySQL, Hive, ODPS, SQLite, support both statements. Translate Database Column Type to TensorFlow Feature Column Type After retrieving database column type name through DatabaseTypeName, we can derive TensorFlow’s feature column type via a mapping such as {&quot;FLOAT&quot;, &quot;DOUBLE&quot;} -&gt; tf.numeric_column. Save Model SQLFlow saves trained ML model by dumping the serialized the model directory into a table. It first creates a table by CREATE TABLE IF NOT EXISTS %s (id INT AUTO_INCREMENT, block BLOB, PRIMARY KEY (id)) and insert blobs by INSERT INTO %s (block) VALUES(?). Note that Hive and ODPS doesn’t have BLOB type, we need to use BINARY (docs at ODPS, Hive) instead. Also, note that Hive and ODPS doesn’t support AUTO_INCREMENT, we need to implemented auto increment logic in sqlfs. Load Model SQLFlow loads trained ML model by reading rows in a table and deserializing the blob to a model directory. It reads rows by running SELECT block FROM %s ORDER BY id, which is supported by most databases. Data Operations in Python Connect to SQL Engines Thanks to the Python database API, connecting to different databases follows a similar API. conn = mysql.connector.connect(user=&#39;scott&#39;, password=&#39;password&#39;, host=&#39;127.0.0.1&#39;, database=&#39;employees&#39;) conn = sqlite3.connect(&#39;path/to/your/sqlite/file&#39;) conn = pyhive.connect(&#39;localhost&#39;) cursor = conn.cursor() cursor.execute(&#39;select * from my_table;&#39;) Insert Prediction Result into Prediction Table Python database API provides execute_many(sql, value) to insert multiple values at once. So one can prepare the following insertion statement. Please be aware that MySQL and SQLite use INSERT INTO to insert rows while Hive and ODPS use INSERT INTO TABLE. -- MySQL, SQLite INSERT INTO table_name VALUES (value1, value2, value3, ...); -- Hive, ODPS INSERT INTO TABLE table_name VALUES (value1, value2, value3, ...);",
    "url": "/doc/database_abstraction_layer.html",
    "relUrl": "/doc/database_abstraction_layer.html"
  },
  "6": {
    "id": "6",
    "title": "Run MySQL Server and Client in Docker Containers",
    "content": "Run MySQL Server and Client in Docker Containers The document explains how to setup MySQL in our development environment. Run MySQL Server in a Docker Container docker run --rm -v /tmp/test1:/var/lib/mysql --name mysql01 -e MYSQL_ROOT_PASSWORD=root -e MYSQL_ROOT_HOST=&#39;%&#39; -p 3306:3306 -d mysql/mysql-server:8.0 the -v option ensures that the database is saved on the host. The default directory where MySQL saves the database is /var/lib/mysql. This directory can be configured in /etc/mysql/my.cnf, as explained in this post. By overlaying the directory /tmp/test1 on the host to /var/lib/mysql, we “cheat” MySQL to save databases on the host. So, we can kill the container and restart it, and the database is still there. Please be aware that the directory on the host must be empty the first time we run the above command; otherwise, MySQL would fail to initialize. I figured out this problem after several failures using docker logs. the -e option sets the root password of MySQL to “root”. Feel free to set it to any password you like. the second -e options sets MYSQL_ROOT_HOST to a wildcard so to allow clients connecting to the server via TCP/IP as the user “root”. This trick works with MySQL 5.7 and 8.0, but not the most recent under-development version. the --name option names the container to mysql01, which can be used to refer to this container. the -p option maps the port 3306, on which the MySQL server listens, to the same port on the host, so that clients could connect to the server via TCP/IP. Run MySQL Client in the Server Container docker exec -it mysql01 mysql -uroot -p This command executes the command mysql, which is the command line tool of MySQL, in the container named mysql01. The command line flags of mysql include -u, which specifies the username of MySQL, and -p, which makes MySQL prompts for the password. For this example, we should type the password “root”, which was set in the previous command. Please wait for a few seconds after the starting of the MySQL server container before we execute the client; otherwise, the startup of the client might fail. Once we get into the MySQL client, we can type SQL commands, e.g., show databases; create database yi; Run Client in a Different Container on the Same Host docker run --rm -it -v /tmp/test1:/var/lib/mysql mysql/mysql-server:8.0 mysql -uroot -p The -v option maps the database directory on the host to the client container. This mapping is necessary because, by default, the client talks to the server via Unix socket /var/lib/mysql/mysql.sock, which is /tmp/test1/mysql.sock on the host. Run Client in a Container on a Remote Host docker run --rm -it mysql/mysql-server:8.0 mysql -h 192.168.1.3 -P 3306 -uroot -p the -h option tells the client where the server is running on. In this example, the given IP is the one of the host where I ran the MySQL server container. Please be aware that the above command works only if the server allows remote connections. Run Python Client in a Container To write a Python client, we need to install the Python package mysql-connector-python. FROM python:2.7 RUN pip install mysql-connector-python Please be aware that some documents says that we need to install mysql-connector. I tried; but the mysql.connector.connect call failed with the error mysql.connector.errors.NotSupportedError: Authentication plugin &#39;caching_sha2_password&#39; is not supported. Build the Docker image: docker build -t sqlflow . Run the image: docker run --rm -it sqlflow bash and we can start Python and run the following Python code snippet &gt;&gt;&gt; import mysql.connector &gt;&gt;&gt; db = mysql.connector.connect(user=&#39;root&#39;, passwd=&#39;root&#39;, host=&#39;192.168.1.3&#39;) &gt;&gt;&gt; print(db) &lt;mysql.connector.connection_cext.CMySQLConnection object at 0x7fbab9f3fed0&gt; Run a Go Client In order to connect to a database, you need to import the database’s driver first. export GOPATH=$HOME/go go get -u github.com/go-sql-driver/mysql go run the following file package main import ( &quot;database/sql&quot; &quot;github.com/go-sql-driver/mysql&quot; &quot;log&quot; ) func main() { testConfig := &amp;mysql.Config{ User: &quot;root&quot;, Passwd: &quot;root&quot;, Net: &quot;tcp&quot;, Addr: &quot;localhost:3306&quot;, } db, e := sql.Open(&quot;mysql&quot;, testConfig.FormatDSN()) if e != nil { log.Fatal(e) } defer db.Close() }",
    "url": "/doc/mysql-setup.html",
    "relUrl": "/doc/mysql-setup.html"
  },
  "7": {
    "id": "7",
    "title": "Piping Responses",
    "content": "Piping Responses Streaming Responses As described in the overall design, a SQLFlow job could be a standard or an extended SQL statemnt, where an extended SQL statement will be translated into a Python program. Therefore, each job might generate up to the following data streams: standard output, where each element is a line of text, standard error, where each element is a line of text, data rows, where the first element consists of fields name/types, and each of the rest is a row of data, status, where the element could be pending, failed, and succeeded. To create good user experience, we need to pipe these responses from SQLFlow jobs to Jupyter Notebook in real-time. Stages in the Pipe The pipe that streams outputs from SQLFlow jobs to the Jupyter Notebook consists of the following stages: Web browser ↑ | HTTP ↓ Jupyter Notebook server ↑ | ZeroMQ streams: Shell, IOPub, stdin, Controls, Heartbeat ↓ iPython kernel ↑ | IPython magic command framework ↓ SQLFlow magic command for Jupyter ↑ | gRPC ↓ SQLFlow server ↑ | Go channels ↓ SQLFlow job manager (Go functions) ↑ | IPC with Go&#39;s standard library ↓ SQLFlow jobs In the above figure, from the SQLFlow magic command to the bottom layer are our work. Streaming We have two alternative ideas: multiple streams and a multiplexing stream. We decided to use a multiplexing stream because we had a unsuccessful trial with the multiple streams idea: we make the job writes to various Go channels and forward each Go channel to a streaming gRPC call, as the following: Multiple streams The above figure shows that there are multiple streams between the Jupyter Notebook server and Jupyter kernels. According to the document, there are five: Shell, IOPub, stdin, Control, and Heartbeat. These streams are ZeroMQ streams. We don’t use ZeroMQ, but we can take the idea of having multiple parallel streams in the pipe. service SQLFlow { rpc File(string sql) returns (int id) {} rpc ReadStdout(int id) returns (stream string) {} rpc ReadStderr(int id) returns (stream string) {} rpc ReadData(int id) returns (stream Row) {} rpc ReadStatus(int id) returns (stream int) {} } However, we realized that if the user doesn’t call any one of the SQLFlow.Read... call, there would be no forwarding from the Go channel to Jupyter, thus the job would block forever at writing. A Multiplexing Stream Another idea is multiplexing all streams into one. For example, we can have only one ZeroMQ stream, where each element is a polymorphic type – could be a text string or a data row. service SQLFlow { rpc Run(string sql) returns (stream Response) {} } // Only one of the following fields should be set. message Response { oneof record { repeated string head = 1; // Column names. repeated google.protobuf.Any row = 2; // Cells in a row. string log = 3; // A line from stderr or stdout. } }",
    "url": "/doc/pipe.html",
    "relUrl": "/doc/pipe.html"
  },
  "8": {
    "id": "8",
    "title": "Quick start",
    "content": "Quick start SQLFlow is currently under active development. For those who are interested in trying it out, we have provided several demos. Play around with it. Any bug report and issue are welcomed. :) Setup Install Docker. Set up a MySQL server following example/datasets/README.md. Pull the latest SQLFlow Docker image: docker pull sqlflow/sqlflow:latest. Demo 1: Jupyter Notebook Start a Docker container that runs sqlflowserver and Jupyter Notebook docker run --rm -it -p 8888:8888 sqlflow/sqlflow:latest bash -c &quot;sqlflowserver --db_user root --db_password root --db_address host.docker.internal:3306 &amp; SQLFLOW_SERVER=localhost:50051 jupyter notebook --ip=0.0.0.0 --port=8888 --allow-root&quot; If you are running MySQL on the localhost and you are using Docker for Mac, please be aware the option --db_address host.docker.internal:3306 where host.docker.internal translates to the host ip address as recommended here. If you are running MySQL on remote, please be aware that MySQL only allows connections from localhost by default. Fix can be found here. Open a Web browser and direct to localhost:8888 and input the token. Then you can create notebooks. In a cell, you should be able to type %%sqlflow select 1 Explore more examples at example.ipynb Demo 2: Command Line Prompt Start a Docker container that runs SQLFlow command line prompt. docker run -it --rm --net=host sqlflow/sqlflow:latest demo --db_user root --db_password root --db_address host.docker.internal:3306 You should be able to see the following prompt. sqlflow&gt; Example Select data sqlflow&gt; select * from iris.train limit 2; -- +--+-+--+-+-+ | SEPAL LENGTH | SEPAL WIDTH | PETAL LENGTH | PETAL WIDTH | CLASS | +--+-+--+-+-+ | 6.4 | 2.8 | 5.6 | 2.2 | 2 | | 5 | 2.3 | 3.3 | 1 | 1 | +--+-+--+-+-+ Train a Tensorflow DNNClassifier sqlflow&gt; SELECT * FROM iris.train TRAIN DNNClassifier WITH n_classes = 3, hidden_units = [10, 20] COLUMN sepal_length, sepal_width, petal_length, petal_width LABEL class INTO sqlflow_models.my_dnn_model; -- ... Training set accuracy: 0.96721 Done training Prediction using a trained model sqlflow&gt; SELECT * FROM iris.test predict iris.predict.class USING sqlflow_models.my_dnn_model; Checkout prediction result sqlflow&gt; select * from iris.predict limit 10;",
    "url": "/doc/quickstart.html",
    "relUrl": "/doc/quickstart.html"
  },
  "9": {
    "id": "9",
    "title": "Extended SQL Parser Design",
    "content": "Extended SQL Parser Design This documentation explains the technical decision made in building a SQL parser in Go. It is used to parsed the extended SELECT syntax of SQL that integrates TensorFlow Estimators. Related Work Lexer and Parser Generator In 2001, when I was in graduate school, I defined an extended SQL syntax for querying distributed relational databases, as part of the course project of Distributed RDBMS by Prof. Li-zhu Zhou. I wrote the parser using bison (a modern version of yacc) and flex (a modern version of lex). yacc and lex generate C code; bison and flex generate C++ code. However, this time, I’d use Go. I surveyed goyacc, a standard Go tool. The usage is very similar to that of yacc and bison. However, the Go toolchain doesn’t provide a tool like lex/flex. Google revealed golex, which is out of maintenance. The Medium post recommends Ragel, which is a C++ program and could generate Go lexer; however, it lacks documents. Handwritten Lexer and Parser Some documents, including this one recommends handwriting lexers. However, it doesn’t explain how to write the parser. GoAcademy always provides high-quality tech blog posts. This one is from the author of InfluxDB. However, I stopped at where it explains wrapping a SQL statement as a string by an io.Reader, because it is obvious that we should keep the string as a string so that that token strings could refer to the same memory storage of the SQL statement. Following a link in the above GoAcademy post, I found Rob Pike’s excellent talk on how to write a lexer in Go in 2011. Many works after that change Rob’s implementation somehow but always lead to longer and less comprehensible codebases. The Choice Therefore, I wrote the lexer and parser both following Rob Pike’s idea. After few days work, I realized that: I should borrow the idea from Rob to represent SQL statements as strings, but not io.Reader as other work do, but no need to use channels and goroutines at all, and it is technically intractable to write a SQL lexer/parser manually. So, I switched to write a lexer manually, and to generate the parser using goyacc. During my work, I referred to this example and the official yacc manual for details about operator association and precedence.",
    "url": "/doc/sql_parser.html",
    "relUrl": "/doc/sql_parser.html"
  },
  "10": {
    "id": "10",
    "title": "Submitter",
    "content": "Submitter A submitter is a pluggable module in SQLFlow that is used to submit an ML job to a third party computation service. Workflow When a user types in an extended SQL statement, SQLFlow first parses and semantically verifies the statement. Then SQLFlow either runs the ML job locally or submits the ML job to a third party computation service. In the latter case, SQLFlow produces a job description (TrainDescription or PredictDescription) and hands it over to the submitter. For a training SQL, SQLFlow produces TrainDescription; for prediction SQL, SQLFlow produces PredDescription. The concrete definition of the description looks like the following type ColumnType struct { Name string // e.g. sepal_length DatabaseTypeName string // e.g. FLOAT } // SELECT * // FROM iris.train // TRAIN DNNClassifier // WITH // n_classes = 3, // hidden_units = [10, 20] // COLUMN sepal_length, sepal_width, petal_length, petal_width // LABEL class // INTO sqlflow_models.my_dnn_model; {% raw %} type TrainDescription struct { StandardSelect string // e.g. SELECT * FROM iris.train Estimator string // e.g. DNNClassifier Attrs map[string]string // e.g. {{&quot;n_classes&quot;, &quot;3&quot;}, {&quot;hidden_units&quot;, &quot;[10, 20]&quot;}} X []ColumnType // e.g. {{&quot;sepal_length&quot;, &quot;FLOAT&quot;}, ...} Y ColumnType // e.g. {&quot;class&quot;, &quot;INT&quot;} ModelName string // e.g. my_dnn_model } {% endraw %} // SELECT * // FROM iris.test // PREDICT iris.predict.class // USING sqlflow_models.my_dnn_model; type PredDescription struct { StandardSelect string // e.g. SELECT * FROM iris.test TableName string // e.g. iris.predict ModelName string // e.g. my_dnn_model } Submitter Interface The submitter interface should provide two functions Train and Predict. The detailed definition can be the following type Submitter interface { // Train executes a ML training job and streams job&#39;s response through writer. // A typical Train function should include // - Loading the training data // - Initializing the model // - model.train // - Saving the trained model to a persistent storage Train(desc TrainDescription, writer PipeWriter) error // Predict executes a ML predicting job and streams job&#39;s response through writer // A typical Predict function should include // - Loading the model from a persistent storage // - Loading the prediction data // - model.predict // - Writing the prediction result to a table Predict(desc PredictDescription, writer PipeWriter) error } Register a submitter A new submitter can be added as import ( &quot;.../my_submitter&quot; &quot;.../sqlflow/sql&quot; ) func main() { // ... sql.Register(my_submitter.NewSubmitter()) // ... for { sql := recv() sql.Run(sql) } } where sql.Register will put my_submitter instance to package level registry. During sql.Run, it will check whether there is a submitter registered. If there is, sql.Run will run either submitter.Train or submitter.Predict.",
    "url": "/doc/submitter.html",
    "relUrl": "/doc/submitter.html"
  },
  "11": {
    "id": "11",
    "title": "SQLFlow: Design Doc",
    "content": "SQLFlow: Design Doc What is SQLFlow SQLFlow is a bridge that connects a SQL engine, for example, MySQL, Hive, SparkSQL, Oracle, or SQL Server, and TensorFlow and other machine learning toolkits. SQLFlow extends the SQL syntax to enable model training and inference. Related Work We could write simple machine learning prediction (or scoring) algorithms in SQL using operators like DOT_PRODUCT. However, this requires copy-n-pasting model parameters from the training program into SQL statements. Some proprietary SQL engines provide extensions to support machine learning. Microsoft SQL Server Microsoft SQL Server has the machine learning service that runs machine learning programs in R or Python as an external script: CREATE PROCEDURE generate_linear_model AS BEGIN EXEC sp_execute_external_script @language = N&#39;R&#39; , @script = N&#39;lrmodel &lt;- rxLinMod(formula = distance ~ speed, data = CarsData); trained_model &lt;- data.frame(payload = as.raw(serialize(lrmodel, connection=NULL)));&#39; , @input_data_1 = N&#39;SELECT [speed], [distance] FROM CarSpeed&#39; , @input_data_1_name = N&#39;CarsData&#39; , @output_data_1_name = N&#39;trained_model&#39; WITH RESULT SETS ((model varbinary(max))); END; A challenge to the users is that they need to know not only SQL but also R or Python, and they must be capable of writing machine learning programs in R or Python. Teradata SQL for DL Teradata also provides a RESTful service, which is callable from the extended SQL SELECT syntax. SELECT * FROM deep_learning_scorer( ON (SELECT * FROM cc_data LIMIT 100) URL(&#39;http://localhost:8000/api/v1/request&#39;) ModelName(&#39;cc&#39;) ModelVersion(&#39;1&#39;) RequestType(&#39;predict&#39;) columns(&#39;v1&#39;, &#39;v2&#39;, ..., &#39;amount&#39;) ) The above syntax couples the deployment of the service (the URL in the above SQL statement) with the algorithm. Google BigQuery Google BigQuery enables machine learning in SQL by introducing the CREATE MODEL statement. CREATE MODEL dataset.model_name OPTIONS(model_type=&#39;linear_reg&#39;, input_label_cols=[&#39;input_label&#39;]) AS SELECT * FROM input_table; Currently, BigQuery only supports two simple models: linear regression and logistic regression. Design Goal None of the above meets our requirement. First of all, we want to build an open source software. Also, we want it to be extensible: We want it extensible to many SQL engines, instead of targeting any one of them. Therefore, we don’t want to build our syntax extension on top of user-defined functions (UDFs); otherwise, we’d have to implement them for each SQL engine. We want the system extensible to support sophisticated machine learning models and toolkits, including TensorFlow for deep learning and xgboost for trees. Another challenge is that we want SQLFlow to be flexible enough to configure and run cutting-edge algorithms, including specifying feature crosses. At the same time, we want SQLFlow easy to learn – at least, no Python or R code embedded in the SQL statements, and integrate hyperparameter estimation. We understand that a key to address the above challenges is the syntax of the SQL extension. To craft a highly-effective and easy-to-learn syntax, we need user feedback and fast iteration. Therefore, we’d start from a prototype that supports only MySQL and TensorFlow. We plan to support more SQL engines and machine learning toolkits later. Design Decisions As the beginning of the iteration, we propose an extension to the SQL SELECT statement. We are not going a new statement way like that BigQuery provides CREATE MODEL, because we want to maintain a loose couple between our system and the underlying SQL engine, and we cannot create the new data type for the SQL engine, like CREATE MODEL requires. We highly appreciate the work of TensorFlow Estimator, a high-level API for deep learning. The basic idea behind Estimator is to implement each deep learning model, and related training/testing/evaluating algorithms as a Python class derived from tf.estimator.Estimator. As we want to keep our SQL syntax simple, we would make the system extensible by calling estimators contributed by machine learning experts and written in Python. The SQL syntax must allow users to set Estimator attributes (parameters of the Python class’ constructor, and those of train, evaluate, or predict). Users can choose to use default values. We have a plan to integrate our hyperparameter estimation research into the system to optimize the default values. Though estimators derived from tf.estimator.Estimator run algorithms as TensorFlow graphs; SQLFlow doesn’t restrict that the underlying machine learning toolkit has to be TensorFlow. Indeed, as long as an estimator provides methods of train, evaluate, and predict, SQLFlow doesn’t care if it calls TensorFlow or xgboost. Precisely, what SQLFlow expect is an interface like the following: class AnEstimatorClass: __init__(self, **kwargs) train(self, **kwargs) evaluate(self, **kwargs) predict(self, **kwargs) We also want to reuse the feature columns API from Estimator, which allows users to columns of tables in a SQL engine to features to the model. Extended SQL Syntax Again, just as the beginning of the iteration, we propose the syntax for training as SELECT * FROM kaggle_credit_fraud_training_data LIMIT 1000 TRAIN DNNClassifier /* a pre-defined TensorFlow estimator, tf.estimator.DNNClassifier */ WITH layers=[100, 200], /* a parameter of the Estimator class constructor */ train.batch_size = 8 /* a parameter of the Estimator.train method */ COLUMN *, /* all columns as raw features */ cross(v1, v9, v28) /* plus a derived (crossed) column */ LABEL class INTO sqlflow_models.my_model_table; /* saves trained model parameters and features into a table */ We see the redundancy of * in two clauses: SELECT and COLUMN. The following alternative can avoid the redundancy, but cannot specify the label. SELECT * /* raw features or the label? */ corss(v1, v9, v28) /* derived featuers */ FROM kaggle_credit_fraud_training_data Please be aware that we save the trained models into tables, instead of a variable maintained by the underlying SQL engine. To invent a new variable type to hold trained models, we’d make our system tightly integrated with the SQL engine, and harms the extensibility to other engines. The result table should include the following information: The estimator name, e.g., DNNClassifier in this case. Estimator attributes, e.g., layer and train.batch_size. The feature mapping, e.g., * and cross(v1, v9, v28). Similarly, to infer the class (fraud or regular), we could SELECT * FROM kaggle_credit_fraud_development_data PREDICT kaggle_credit_fraud_development_data.class USING sqlflow_models.my_model_table; System Architecture A Conceptual Overview In the prototype, we use the following architecture: SQL statement -&gt; our SQL parser --standard SQL-&gt; MySQL -extended SQL-&gt; code generator -&gt; execution engine In the prototype, the code generator generates a Python program that trains or predicts. In either case, it retrieves the data from MySQL via MySQL Connector Python API, optionally, retrieves the model from MySQL, trains the model or predicts using the trained model by calling the user specified TensorFlow estimator, and writes the trained model or prediction results into a table. Working with Jupyter Notebook and Kubernetes The following figures shows the system components and their runtime environment. The left part shows how to run the system on a PC/laptop, the right part shows how to run it on a Kubernetes cluster.",
    "url": "/doc/syntax.html",
    "relUrl": "/doc/syntax.html"
  },
  "12": {
    "id": "12",
    "title": "SQLFlow: Code Walkthrough",
    "content": "SQLFlow: Code Walkthrough User Experience SQLFlow allows users to write SQL programs with extended syntax in Jupyter Notebook or a command-line tool. The following SQL statements train a TensorFlow model named DNNClassifier, which is a Python class derived from tf.estimator.Estimator: SELECT * FROM a_table TRAIN DNNClassifier WITH learning_rate=0.01 INTO sqlflow_models.my_model; And the following statement uses the trained model for prediction. SELECT * FROM b_table PREDICT b_table.predicted_label USING sqlflow_models.my_model; Please be aware that the part in the above statements before the extended keyword TRAIN and PREDICT is a standard SQL statement. This feature simplifies the implementation of the SQLFlow system. System Implementation If a SQL statement is of the standard syntax, SQLFlow throws it to the SQL engine and redirects the output to the user; otherwise, SQLFlow translates the statement of extended syntax into a Python program. Currently, it generates a program that throws the standard-syntax part of SELECT to MySQL, reads the results in the train-loop of a TensorFlow program. We will talk about how to extend SQLFlow to connect more SQL engines like Oracle, Hive, and SparkSQL, and generates more types of machine learning programs that calls distributed TensorFlow, PyTorch, and xgboost later. Before that, let us explain the system components. SQLFlow as a gRPC Server SQLFlow is a gRPC server, which can connect with multiple clients. A typical client is pysqlflow, the SQLFlow plugin for Jupyter Notebook server. Another once is a text-based client /cmd/sqlflowserver/main.go. Jupyter Notebook (SQL statements)--&gt; SQLFlow gRPC server (SQLFlow magic command) &lt;--(a stream of messages)-- The protobuf definition of the gRPC service is at /server/proto/sqlflow.proto. The return of the method SQLFlow.Run is a stream of Reponses, where each represents either a table header, a row, or a log message. The header and rows are usually from a standard SQL statement, for example, SELECT or DESCRIBE, and the log messages are usually from the running of a generated Python program. SQLFlow in the gRPC Server Once the SQLFlow server receives a batch of SQL statements via a gRPC call, it runs the following steps for each statement: the parser to generate parsing result, the verifier to verify the semantics given the parsing result, the code generator to generate a Python program, or the submitter, from the parsing result, the executor that runs the submitter locally. Step 3. and 4. are only for a SQL statement of extended syntax; otherwise, SQLFlow server proxies the standard-syntax statement to the SQL engine. The executor calls Go’s standard package that captures the stdout and stderr from the submitter process and passing the result back to the gRPC client. Therefore, it is the responsibility of the submitter to print log messages to its stderr and stdout. Minimal Viable Product In the minimal viable product (MVP) of SQLFlow, the code generator generates a Python program consists of two parts: throw the standard SELECT part in the extended-syntax statement to MySQL via ODBC, and a loop that reads outputs from the run of the SELECT statement and trains the model (or, using a trained model to predict). The training part calls TensorFlow to update the parameters of the model specified in the TRAIN clause. Extensibility By writing more code generators, we could extend SQLFlow to support more SQL engines, e.g., Hive and Oracle, and use machine learning toolkits, e.g., PyTorch and xgboost, in addition to TensorFlow, on various computing platforms. You are welcome to add more code generators such as codegen_distributed_tf.go to generate a submitter program similar to the MVP but runs a distributed TensorFlow training job. codegen_kubernetes_tf.go to launch a distributed TensorFlow job on a Kubernetes cluster, other than running locally, in the same container as where SQLFlow gRPC server resides. codegen_gcloud_pytorch.go to launch a submitter that calls PyTorch instead of TensorFlow for training on the Google Cloud. Job Management The current design of the gRPC interface assumes that the connection between the client, e.g., the Jupyter Notebook, and the SQLFlow server keeps alive during the running of the training program. This assumption is reasonable because even if the user closes her/his Web browser and disconnect to the Jupyter Notebook server, the connection between Jupyter to SQLFlow server might keep alive. However, this might not be robust enough if the Jupyter Notebook server runs on a user’s laptop and gets killed. In such a case, the gRPC server cannot stream the messages back to the client and would cause the failure of the submitter. A solution is to change the gRPC interface of SQLFlow server to have a method that files a job and returns immediately, and another method to get a batch of recent messages given a job ID. We will make a design for that soon.",
    "url": "/doc/walkthrough.html",
    "relUrl": "/doc/walkthrough.html"
  }
  
}
